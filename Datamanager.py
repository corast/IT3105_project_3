# Keep track of training data to use on the network, i.e. keep history of our plays.

# We need to store this incase we close our program, and want to keep training.
# Should be storer in a .csv file. for easy access next time.
# .CSV file should contain: A state input, Player ID , + Output on all states.  dimxdim + PID input + dimxdim output.
# The output of all states should be visits on all legal states, generated by the MCTS simulation
# * Replay buffer: each episode(game) contains m moves, which mean m cases for the replay buffer
# * The state input, should be two bits for each board cell. (0,0) for 0, (1,0) for player 1 and (1,1) for player 2.
# The PID should be two bits: (1,0) for player 1 and (1,1) for player 2.

import csv # Gives us the oppurtunity to write to an file.
import numpy as np
import random
import torch
import os
#import pandas
class Datamanager():
    def __init__(self, filepath, dim=5):
        self.filepath = filepath
        #if(not os.path.isfile(filepath)): # if filepath is not a file

        #    raise ValueError("filepath is not a path")

        self.inputs = 2+dim*dim*2
        self.outputs = dim*dim
    
    # TODO: specify an random amount as cases
    # TODO: use dictionaries instead?

    def read_csv(self,header=False): # amount is to specify whether or not we want all the file or not.
        # read the content of the csv file.
        file = []
        with open(self.filepath) as csv_file:
            csv_reader = csv.reader(csv_file,delimiter=",")

            line_count = 0
            for row in csv_reader:
                if(header and line_count == 0): # Handle header
                    #print("headers: {}".format(row))
                    line_count += 1
                else:
                    file.append(row)
                    #print(row)
                line_count += 1
        return file

    def update_csv(self,mode="a",header=[],data=[[]]): # Data should be an array of arrays, with all the data we need
        # The array must be [[row1],[row2]] etc. not [row1]
        # []
        # Mode specify if we want to replace the file with data parameter, or append new data.
        with open(self.filepath, mode=mode, newline="") as history_file:
            # Init writer to the file, with dialect
            dataset_writer = csv.writer(history_file, delimiter=",", quotechar='"', quoting=csv.QUOTE_MINIMAL)
            if(len(header) != 0):
                dataset_writer.writerow(header)
            # Write the actual data.
            for row in data:
                dataset_writer.writerow(row)

    def update_csv_limit(self,header=[],data=[[]],limit=1000): 
        # Update the csv file, but only keep the top 1000 newest examples.
        # We need to remove from the bottom.
        #TODO: normalize the target function
        if(len(header) != 0):
            old_data = self.read_csv(header=True)
        else:
            old_data = self.read_csv(header=False)

        old_data.extend(data)
        new_data = old_data

        len_dataset = len(new_data)
        len_row = len(data[0]) # assume there is something from before.

        if(len_dataset > limit): # If we have more data than we want, remove top rows.
            amount = len_dataset - limit
            new_data = new_data[amount:] # We want to only keep from amount and out.

        if(len(header) != 0):
            self.update_csv(mode="w",header=header,data=new_data)
        else:
            self.update_csv(mode="w",data=new_data)

    def update_buffer(self):
        pass
    
    def return_num(self, num, tot_size):
        """ Return number of cases we want to keep. """
        # Either a fraction, all cases or a specific number.
        if(num != "all"):
            #check if an integer of a float.
            if(type(num) == float):  # if float
                if(num > 1):
                    num_floor = np.floor(num)
                    num = num - num_floor # only keep decimal places
                num = int(tot_size*num) # Keep only the specified fraction.

            if(tot_size < num):    
                num = tot_size
            return num
        return tot_size # Know we want all cases.

    def return_batch(self, batch_size): # Return tensor with batch_size from file.
        """ Return two tensors(inputs, targets) of size batch_size from bufferfile"""
        data = self.read_csv()
        tot_size = len(data) # Number of cases we got.
        # Num, is the amount of data we send back.
        num = self.return_num(batch_size, tot_size)
        if(tot_size == 0):
            raise ValueError("No cases availible in file")
        
        # Randomly select num unique cases
        data = random.sample(data, num) # We don't care about testing or validation at this stage

        targets = []
        for i, row in enumerate(data):
            data[i] = list(map(float, row))
            inputs = data[i][:self.inputs]
            target = data[i][self.inputs:]
            # We need to seperate input and target into two seperate lists
            data[i] = inputs
            targets.append(target)
        inputs = data
        #print("targets",targets)
        t_inputs = torch.from_numpy(np.array(inputs)).float()
        t_targets = torch.from_numpy(np.array(targets)).long()

        return t_inputs, t_targets

    def get_buffer_size(self):
        r""" return number of rows in csv file"""
        data = self.read_csv()
        return len(data)


def test_return_batch():
    dataset = Datamanager("Data/data_r_test.csv",5)
    dataset.return_batch(10)

#test_dataset_write()